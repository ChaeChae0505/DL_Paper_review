# DL Paper_review
- Paper review ë° model êµ¬í˜„ ë‚´ìš©ì— ëŒ€í•´ ìž‘ì„±í•©ë‹ˆë‹¤.
- ëª¨ë¸ êµ¬í˜„ ! ì€ TIL_DL_NEW/ Models / vit / modelname.ipynb ë¡œ ë³´ë‚´ê³ , READMEì—ë‹¤ ì •ë¦¬í•˜ìž
---
### To-Do List 
|ê³µë¶€ê¸°ê°„|Model|Paper|Code|
|:----|:----|:---:|:------:|
|220105 ~|[ViT](https://arxiv.org/pdf/2010.11929.pdf)|âŒ|âŒ|
||
ðŸ˜†âœ”ï¸âœ”ï¸
ðŸ™‚âŒâœ”ï¸
ðŸ˜¡âŒâŒ

---
# Pytorch TIP
- ê¸°íƒ€ pytorch tipì€ TIL_DL_NEW/README.md ì— ì ìž
  - pytorch model ìƒì„± íŒ ì°¸ê³ 
  - [1][https://gaussian37.github.io/dl-pytorch-snippets/](https://gaussian37.github.io/dl-pytorch-snippets/)
  - [2][https://anweh.tistory.com/21](https://anweh.tistory.com/21)
  - [3][ì˜ˆì œë¡œ ë°°ìš°ëŠ” pytorch](https://tutorials.pytorch.kr/beginner/pytorch_with_examples.html)
  - [4][Pytorch í™œì„±í™” í•¨ìˆ˜ì˜ ì¢…ë¥˜](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)
  - [5][paper review](https://github.com/Seonghoon-Yu/AI_Paper_Review)

## ëª©ì°¨  
[ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜í•˜ëŠ” ë°©ë²•](#ì‹ ê²½ë§-ëª¨ë¸-ì •ì˜í•˜ëŠ”-ë°©ë²•)  
[ì°¨ì›ê´€ë¦¬einops](#ì°¨ì›ê´€ë¦¬)  
[torch.cat](#torch-cat)  

---

## ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜í•˜ëŠ” ë°©ë²• 
- [ì°¸ê³ 1](https://anweh.tistory.com/21)
- [ì°¸ê³ 2](https://data-panic.tistory.com/15)
- [wikidocs pytorchë¡œ ì‹œìž‘í•˜ëŠ” ë”¥ëŸ¬ë‹ ìž…ë¬¸](https://wikidocs.net/60036)
```
@ Pytorch ì‹ ê²½ë§ ì„¤ê³„ ë°©ë²•
1. ì‚¬ìš©ìž ì •ì˜ nn ëª¨ë“ˆ
2. nn.Module ìƒì†í•œ í´ëž˜ìŠ¤ ì´ìš©
- classë¡œ ì„ ì–¸í•  ë•Œ __init__ê³¼ forwardëŠ” í•„ìˆ˜!
- forwardì—ì„œ H(x)ì— ìž…ë ¥ xë¡œ ë¶€í„° ì˜ˆì¸¡ëœ ê²°ê³¼ yë¥¼ ì–»ê²Œ í•´ì¤Œ!


@ pytorchë¡œ ì‹ ê²½ë§ ëª¨ë¸ ì„¤ê³„ ì‹œ Step
1. Design your model using class with Variables
2. Construct loss and optim
3. Train cycle(forward, backwoard, update)
```

## nn.Sequential ê³¼ nn.ModuleList ì°¨ì´
- ì´í•´ê°€ ì–´ë µë‹¤ ëŒ€ì¶© ì´í•´ í–ˆì„ ë•Œ sequentialì€ ëª¨ë¸ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ í•©ì¹  ìˆ˜ ìžˆëŠ” ë“¯ í•˜ë‹¤ [ì°¸ê³ ](https://michigusa-nlp.tistory.com/26) ë‹¤ì‹œ ì½ì–´ë³´ìž

## ì°¨ì›ê´€ë¦¬
- ViT codeë¥¼ ë³´ë‹¤ ì•Œê²Œ ë˜ì—ˆë‹¤.
```
from einops import rearrange, reduce, repeat
# rearrange elements according to the pattern
output_tensor = rearrange(input_tensor, 't b c -> b c t')
# combine rearrangement and reduction
output_tensor = reduce(input_tensor, 'b c (h h2) (w w2) -> b h w c', 'mean', h2=2, w2=2)
# copy along a new axis 
output_tensor = repeat(input_tensor, 'h w -> h w c', c=3)
```
- strë¡œ ì°¨ì› ê´€ë¦¬ë¥¼ í•œë‹¤ëŠ”ê²Œ ìµìˆ™í•˜ì§€ëŠ” ì•Šì§€ë§Œ, ê·¸ëž˜ë„ ì—„ì²­ ì§ê´€ì ì¸ ê²ƒ ê°™ë‹¤

## torch cat
- casting í•˜ëŠ” ì¡´ìž¬ì¸ ê±° ê°™ë‹¤
> torch.cat(tensors, dim=0, *, out=None) 

```
Parameters
tensors (sequence of Tensors) â€“ any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension.

dim (int, optional) â€“ the dimension over which the tensors are concatenated

Keyword Arguments
out (Tensor, optional) â€“ the output tensor.
```

```python
>>> x = torch.randn(2, 3)
>>> x
tensor([[ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497]])
>>> torch.cat((x, x, x), 0)
tensor([[ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497],
        [ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497],
        [ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497]])
>>> torch.cat((x, x, x), 1)
tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,
         -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,
         -0.5790,  0.1497]])
```
